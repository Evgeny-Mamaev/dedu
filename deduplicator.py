import itertools
import json
import os
import sys
import queue
from threading import Thread
from typing import Any

from fuzzywuzzy import fuzz
from sortedcontainers import SortedSet
from collections import Counter
from termcolor import colored

from kafkaadapter import consume_messages
from mongoadapter import read_data

from redisadapter import RedisDict, RULE_TO_SET_DICT

INDEX_REFS = 'index_refs'
SNAPSHOTS = 'snapshots'
EDGES = 'edges'
DFS_SNAPSHOTS = 'dfs_snapshots'
REF_TO_MASTER = 'ref_to_master'
UUID = 'uuid'
LIST = 'l_'
AUX_PARAMS = [SNAPSHOTS, DFS_SNAPSHOTS, REF_TO_MASTER, INDEX_REFS]
KEY_SUFFIX = '00'
LEVENSHTEIN_TOLERANCE = 71

MERGES_FILE = './notebook/merges.json'

q = queue.Queue()


def read_data_recursively(skip, limit):
    """
    This method makes ahead-of-time data retrieval from the DB. It works in parallel with the main computational logic.
    :param skip: how many records to skip in this round.
    :param limit: how many records to include in this retrieval.
    :return: nothing but a side effect of populating the q queue with the retrieved data.
    """
    data = {}
    read_data(skip, limit, data)
    q.put(data)
    if len(data) == 0:
        return
    skip += limit
    read_data_recursively(skip, limit)


def run(strategy: str):
    """
    This is the main method, it cyclically reads the command prompt and starts off either the process of deduplication
    of a list (or a single one) of snapshots, or the process of changing of the previously processed snapshot,
    or simply adding another portion of snapshots to the existing deduplicated state.

    This method emulates the three possible workflows corresponding to the paragraph above:
    1. Building a new index out of a list of the previously collected and not yet processed snapshots (an initial run);
    2. Making a change to an already processed snapshot present in the deduplicated state (breaking up the master and
    re-processing the split collection);
    3. An ordinary processing of snapshots one by one or in the batch mode (generally the pathway is applicable to a
    single snapshot as the business requirement doesn't suggest any batch-mode cases, the latter is introduced here for
    the sake of completeness).

    :param strategy: this implies that the strategy is supplied as the command line argument.
    :return: it doesn't return anything as all it does is produces side effects such as printing, and changes its state.
    """
    remove_merges_file()

    print('Generate snapshots first with \'python3 generator.py 10000 3 5 55 data.json new\'.')
    print('Use here \'data.json\' or another generated by the \'generator.py\' file.')
    indices = dict()
    merges = dict()
    init(indices, strategy, merges)

    snapshots_dict = RedisDict('s__', False)
    is_to_change = False
    entities_in_indices = set()
    prev_merge_counter = 0
    skip = 0
    window = 0
    path = None
    is_to_read_input = True
    while True:
        if is_to_read_input:
            skip = 0
            path = get_input('Enter a path to the file with snapshots to add or type in \'change\' or \'db\' '
                             'or \'redis\' or \'matches\' or \'transactional\': ')
        if path == 'exit':
            counter = count_matches(strategy, snapshots_dict)
            print('matches: ' + str(counter))
            break
        if path == 'change':
            is_to_change = True
            continue
        if path == 'matches':
            counter = count_matches(strategy, snapshots_dict)
            print('matches: ' + str(counter))
            count_unique_master_records_and_indices(indices, snapshots_dict)
            break
        json_data = {}
        if path == 'transactional' or path == '':
            is_to_read_input = False
            consume_messages(json_data)
        if path == 'db':
            if is_to_read_input:
                skip = int(get_input('Enter a number of records to skip: '))
                window = int(get_input('Enter a rolling window size: '))
            if skip == 0:
                t = Thread(target=read_data_recursively, args=[skip, window])
                t.start()
            json_data = q.get()
            skip += window
            is_to_read_input = False
            if len(json_data) == 0:
                is_to_read_input = True
        elif path != 'transactional' and path != '':
            with open(path) as json_file:
                json_str = json_file.read()
                json_data = json.loads(json_str)
                json_data = convert_lists_to_sets(json_data)
        if is_to_change:
            json_data, unmerge_counter = break_up_masters(indices, json_data, snapshots_dict, entities_in_indices)
            prev_merge_counter -= unmerge_counter
            is_to_change = False

        put_open_bracket(skip)

        indices, _, _, entities_in_indices, prev_merge_counter = \
            deduplicate(json_data, indices, snapshots_dict, entities_in_indices, prev_merge_counter, merges)

        put_close_bracket()

        snapshots_dict.sync()
        for index in indices.values():
            index.sync()


def remove_merges_file():
    try:
        os.remove(MERGES_FILE)
    except OSError:
        pass


def put_open_bracket(skip):
    with open(MERGES_FILE, mode='a', encoding='utf-8') as f:
        f.write('[\n')
        json.dump({'snapshotProcessedNumber': skip}, f, indent=4)
        f.write(',\n')


def put_close_bracket():
    with open(MERGES_FILE, mode='ab+') as f:
        f.seek(-2, os.SEEK_END)
        f.truncate()
        f.write(b'\n],\n')


def init(indices: dict[str, RedisDict], strategy: str, merges: dict[str, int]):
    """
    Creates a high-level dictionary with a number of indices corresponding to the number of rules in the strategy.
    :param indices: a high-level dictionary to populate with Redis-based persistent mappings.
    :param strategy: a list of semicolon-delimited rules.
    :param merges: a merge counter by rule.
    :return: nothing but side effect of populating the dictionary.
    """
    if not indices:
        for rule in strategy.split(";"):
            merges[rule] = 0
            RULE_TO_SET_DICT[rule] = SortedSet(rule.split(','))
            indices[rule] = RedisDict(rule + '__')

def get_input(text: str):
    """
    This is simply a wrapper around a built-in function.
    :param text: the text for the command prompt.
    :return: an input string.
    """
    return input(text)


def break_up_masters(indices: dict[str, RedisDict], json_data: dict[str, dict], snapshots_dict: RedisDict[str, dict],
                     entities_in_indices: set[str]):
    """
    When a particular snapshot's changed it's necessary to break up the master it belongs to, as all the connections and
    previous matches within the master become stale. Picture a master as a connected graph and a set of masters as a
    disconnected graph https://en.wikipedia.org/wiki/Connectivity_(graph_theory). None of the masters has connectivity
    to the other. Hence, if a master is being broken up, the next step for its pieces (snapshots) is to form a new set
    of masters (possibly the same single master).

    Imagine all the masters and snapshots raise their multiple imaginary hands marked with coloured ribbons as a sign of
    readiness to be connected. A connection happens if two or more of them have hands with ribbons of the same colour.
    As a step either a snapshot or a master connects to another snapshot or master forming a bigger master. This step
    may be repeated with the same ribbon colour multiple times while there are still hands available, thus connecting
    all the hands with ribbons of the same colour. In these terms the hands held together are a vertex of a graph. As
    pointed out in the methods below, the graph is a net, hence the representation vertices = hands, edges = entities
    (masters and snapshots) is bijective to vertices = entities, edges = hands. In other words, the two structures are
    equivalent according to construction of a line graph (https://en.wikipedia.org/wiki/Line_graph).

    The deduplication step can either connect all the broken pieces back again, hence nothing's changed. The
    result is the same connected graph as before the breakup. Or it can stop some parts of the previously connected
    graph from connecting back again as the changed snapshot no longer holds the affinity to some of its counterparts.
    This scenario ends up with a set of smaller masters. Or there's another alternative. Note that as the snapshot's
    been changed it may acquire such a property that it becomes a bridge between the previously disconnected (by the
    definition) masters. Its new hand can wear a ribbon bracelet of popular colour. It means either the whole previously
    split master gets a new connection with another master becoming a part of it forming an even bigger blob. Or if
    the master's lost some of its connectivity, a new smaller piece through the changed snapshot becomes a part of a
    larger master with which the pre-destroyed masted hasn't had any connectivity, leaving us with this master and
    another remains of the destroyed master.

    :param indices: are the main concept, the indices are being changed during the process of breaking up masters.
    :param json_data: the change, which is to be processed and applied to the masters containing snapshots.
    :param snapshots_dict: a key-value association between UUID and a link to the corresponding snapshot.
    :param entities_in_indices: a set of UUIDs which comprise indices.
    :return: an enumerated list of snapshots which are the broken pieces and an unmerge counter which serves as a check.
    Also, it produces side effects which are described in the corresponding methods.
    """
    all_snapshots_to_reprocess = dict()
    total_unmerge_counter = 0
    for entity in json_data.values():
        snapshots_to_reprocess, unmerge_counter = break_up_master(entity, indices, snapshots_dict, entities_in_indices)
        all_snapshots_to_reprocess |= snapshots_to_reprocess
        total_unmerge_counter += unmerge_counter
    return {k: v for k, v in enumerate(all_snapshots_to_reprocess.values())}, total_unmerge_counter


def break_up_master(entity: dict[str, Any], indices: dict[str, RedisDict], snapshots_dict: RedisDict[str, dict],
                    entities_in_indices: set[str]):
    """
    The data structure which represents the entity is formally a network (https://en.wikipedia.org/wiki/Network_theory).
    But it bears strong resemblance with a tree (https://en.wikipedia.org/wiki/Tree_(data_structure)) in the sense that
    there are a root (master), hence the tree, and nodes (snapshots). The graph is directed. Nodes are both referenced
    by and reference to another nodes. One node can be referenced by and reference to several other nodes, hence the
    net and not the pure tree.

    This method first finds the root of the connected graph, traversing up to the node which isn't referenced by any
    other node within the connectivity it belongs to (namely the root by definition).

    Then it recursively breaks up the graph cutting all the connections, leaving behind separate snapshots as they were
    when being previously involved in forming the graph (master).

    E.g. if the graph (master) consisted of 51 snapshots, the process of splitting it up results in the 51 snapshots.

    :param entity: a changed snapshot, it should be guaranteed by the supplier that the original snapshot is already
    present in the index.
    :param indices: are the main concept, the indices are being changed during the process of breaking up the master.
    :param snapshots_dict: a key-value association between UUID and a link to the corresponding snapshot.
    :param entities_in_indices: a set of UUIDs which comprise indices.
    :return: an enumerated list of snapshots which are the broken pieces and an unmerge counter which serves as a check.
    """
    entity_to_delete_indices_for = stale_entity = snapshots_dict[entity[UUID]]
    while entity_to_delete_indices_for.get(REF_TO_MASTER, None):
        entity_to_delete_indices_for = snapshots_dict[entity_to_delete_indices_for[REF_TO_MASTER]]
    remove_keys_from_indices(indices, entity_to_delete_indices_for)
    snapshots_to_reprocess = []
    unmerge_counter = dfs_snapshots_to_reprocess(entity_to_delete_indices_for, snapshots_to_reprocess, stale_entity,
                                                 entities_in_indices, snapshots_dict)
    unmerge_counter -= 1
    snapshots_dict[entity[UUID]] = entity
    snapshots_to_reprocess.append(entity)
    return dict((v[UUID], v) for v in snapshots_to_reprocess), unmerge_counter


def dfs_snapshots_to_reprocess(entity: dict[str, Any], snapshots_to_reprocess: list[dict], stale_entity: dict[str, Any],
                               entities_in_indices: set[str], snapshots_dict: RedisDict[str, dict]):
    """
    This method gathers all the snapshots of the decaying master in one place snapshots_to_reprocess. This is the core
    of the splitting process.

    :param entity: a current entity to break up.
    :param snapshots_to_reprocess: a container where to put all the snapshots as a result of the splitting process.
    :param stale_entity: a stale entity to compare the candidate to, it serves the aim of skipping the master from
    processing.
    :param entities_in_indices: the method mutates this data structure for the sake of data consistency. The broken
    pieces by definition become snapshots. The snapshots are sent to the next round of deduplication where they'll be
    put into index, thus here they're removed from both the index, and it's view entities_in_indices.
    :param snapshots_dict: a key-value association between UUID and a link to the corresponding snapshot.
    :return: an unmerge counter in the sake of data consistency for the further corrections to the main merge counter.
    """
    unmerge_counter = 1
    for e_id in entity.get(SNAPSHOTS, []):
        e = snapshots_dict[e_id]
        unmerge_counter += dfs_snapshots_to_reprocess(e, snapshots_to_reprocess, stale_entity, entities_in_indices,
                                                      snapshots_dict)
        merge_or_unmerge_values_for_each_key(e, entity, False)
        snapshots_dict[e[UUID]] = e
    if entity != stale_entity:
        snapshots_to_reprocess.append(entity)
    for param in AUX_PARAMS:
        entity.pop(param, None)
    entities_in_indices.discard(entity[UUID])
    return unmerge_counter


def deduplicate(json_data: dict[str, dict], indices: dict[str, RedisDict], snapshots_dict: RedisDict[str, dict],
                prev_entities_in_indices: set[str], prev_merge_counter: int, merges: dict[str, int]):
    """
    This is the kernel of the algorithm. It iterates over indices merging entities which are grouped around the same set
    of parameters (hands with ribbons of the same colour) while possible merges exist. It stops when all the
    possibilities have been utilized.

    The algorithm has an O(n * log ^ m (n)) time and O(n ^ 2) space complexity. This bound doesn't introduce any
    meaningfulness as the practicality of the algorithm takes place only for the sparsely matching data. It perfectly
    reflects the real data profile. Note that the combinatorial explosion exceeds any human-comparable amount of time
    when the formula a ^ b (power) approaches 10 ^ 15 = 10 ^ 10 * 100000 = 100000 secs * 10 GHz = 1 day * 10 GHz for the
    modern CPU, not considering huge parallelism of GPU of order of 10 ^ 5. The present algorithm is not parallel,
    although it is parallelizable (a - the number of values in the merged entity, b - the number of parameters in the
    rule).

    Effectively the algorithm becomes probabilistic in the sense it's based on the relation of the parameters of the
    input data. A single piece of data (person data in particular) is stochastic, but a considerable number (millions)
    of data records becomes a statistical data structure (holding properties of statistical nature).

    The real production data holds such characteristics, skipping mathematical reasoning and considering its sparseness,
    that only 6% of entities are being merged. It claims the time complexity of Î¸(n * log ^ 2 (n)). The lower bound is
    trivially o(n). The mathematical proof for the claimed average time complexity is to be presented.

    In order to provide the user with the confidence in data consistency, each round and approach of the algorithm is
    checked against the equality of the sum of merges have been done and the number of masters left in the indices to
    the number of initial masters in the indices. It doesn't provide any guarantees of correctness or convergence of the
    merges themselves though, as it assures only the absense of unaccounted or extra merges.

    :param json_data: the input data.
    :param indices: this is a mutable data structure which is supplied from the previous approaches of processing the
    batch data.
    :param snapshots_dict: this is an association of UUIDs to snapshots.
    :param prev_entities_in_indices: this is to avoid iteration over the indices form the previous approaches, it's
    used for data consistency.
    :param prev_merge_counter: it's used for data consistency to be able to count all the merges from the previous
    approach of processing batch data.
    :param merges: a merge counter by rule.
    :return: all the above parameters as they can be used in another approach of batch processing.
    """
    total_len = len(json_data) + len(snapshots_dict)
    current_len = len(json_data)

    indices, entities_in_indices, _ = build_indices(json_data, indices, snapshots_dict)

    entities_in_indices |= prev_entities_in_indices

    new_objects_number = len(json_data)
    print('total_len: ' + str(total_len))
    print('current_len: ' + str(current_len))
    print()

    cycle_counter = 0
    total_merge_counter = 0
    total_iterated_over_set = set()

    this_merges = {}

    while True:
        cycle_counter += 1

        merge_counter, iterated_over_set = iterate_over_indices(indices, snapshots_dict, merges, this_merges)
        master_records_set, indices_count, all_snapshots_set_len, all_snapshots_including_masters_set_len, index_len = \
            count_unique_master_records_and_indices(indices, snapshots_dict)
        total_merge_counter += merge_counter
        total_iterated_over_set |= iterated_over_set

        print('All the rules in the strategy, master_records_set_len: ' + str(len(master_records_set)) +
              ', indices_count: ' + str(indices_count))
        print('merge_counter: ' + str(merge_counter))
        print()
        print('merges by rule total: ')
        print(json.dumps(merges, indent = 4))
        print()
        print('merges by rule this round: ')
        print(json.dumps(this_merges, indent = 4))
        print()

        with open(MERGES_FILE, mode='a', encoding='utf-8') as f:
            json.dump(this_merges, f, indent=4)
            f.write(',\n')

        objects_number = len(master_records_set)
        if new_objects_number == objects_number:
            break
        else:
            new_objects_number = objects_number

    total_merge_counter += prev_merge_counter
    print('cycle_counter: ' + str(cycle_counter))
    print()
    print('index_len: ' + str(index_len))
    print('total_len: ' + str(total_len))
    print('current_len: ' + str(current_len))
    print('entities_in_indices: ' + str(len(entities_in_indices)))
    print('total_iterated_over_set: ' + str(len(total_iterated_over_set)))
    print('total_merge_counter: ' + str(total_merge_counter))
    print('master_records_set_len: ' + str(len(master_records_set)))
    print('all_snapshots_set_len: ' + str(all_snapshots_set_len))
    print('all_snapshots_including_masters_set_len: ' + str(all_snapshots_including_masters_set_len))
    print()
    indices_entities_set = set()
    count_entities(indices, indices_entities_set, snapshots_dict)
    print('indices_entities_set: ' + str(len(indices_entities_set)))
    if len(master_records_set) + total_merge_counter == len(entities_in_indices):
        print(colored('OK!', 'white', 'on_green', attrs=['bold']))
    else:
        print(colored('ERROR!', 'white', 'on_red', attrs=['bold']))
    return indices, len(master_records_set) + total_merge_counter, len(entities_in_indices), entities_in_indices, \
        total_merge_counter


def count_entities(indices: dict[str, RedisDict], entities_set: set[str], snapshots_dict: RedisDict[str, dict]):
    """
    This method counts entities in the indices.

    Actually, it can be optimized and be got rid of as the count can be performed alongside with the main traversal
    through the indices. Although it provides a possibility for crosschecking between its results and the results of the
    merge_counter in iterate_over_indices().

    :param indices: the entities are counted traversing this dictionary.
    :param entities_set: a mutable set which is used as a container and being populated by the method.
    :param snapshots_dict: this is an association of UUIDs to snapshots.
    :return: only a side effect of populating entities_set.
    """
    for index in indices.values():
        for entity_id in index.values():
            entities_set.add(entity_id)
            entity = snapshots_dict[entity_id]
            for e in entity.get(DFS_SNAPSHOTS, []):
                entities_set.add(e)


def count_matches(strategy: str, snapshots_dict: RedisDict[str, dict]):
    """
    A naive method for approximate counting of possible matches. It counts simple matches without expanding the process
    further to an extent where there may pop up new merging opportunities after some entities have been merged.
    :param strategy: a set of rules by which the deduplication is to be performed.
    :param snapshots_dict: this is an association of UUIDs to snapshots.
    :return: a match counter.
    """
    rules = []
    for rule in strategy.split(';'):
        a = []
        for param in rule.split(','):
            a.append(param)
        rules.append(a)

    counter = 0
    for entity_left in snapshots_dict.values():
        for entity_right in snapshots_dict.values():
            for rule in rules:
                match = True
                for param in rule:
                    if not entity_left.get(param, None) or not entity_right.get(param, None):
                        match = False
                        break
                    if isinstance(entity_left[param], str) and isinstance(entity_right[param], str) and \
                            fuzz.token_sort_ratio(entity_left[param], entity_right[param]) > LEVENSHTEIN_TOLERANCE:
                        match = False
                        break
                    if entity_left[param] != entity_right[param]:
                        match = False
                        break
                if match:
                    counter += 1
    return counter


def build_indices(json_data: dict[str, dict], indices: dict[str, RedisDict], snapshots_dict: RedisDict[str, dict]):
    """
    This method generates indices for all the entities fed into the program. An index by definition is a sorted
    collection and all the method does is it creates an exponential amount of references to the entity representing each
    combination of parameters according to the rules. One rule - one index, there can be many rules in the strategy.

    :param json_data: the input data.
    :param indices: this is a central abstraction in this method in particular, it's being populated by the method.
    :param snapshots_dict: this is an association of UUIDs to snapshots.
    :return: the populated indices, a set of UUIDs in the indices, and skipped entities which didn't get into the index.
    """
    entities_in_indices = set()
    skipped_entities = set()

    temp_dict = dict()
    for key in json_data:
        snapshot = json_data[key]
        snapshots_dict[snapshot[UUID]] = snapshot

        for param in snapshot:
            value = snapshot[param]
            if param.endswith(KEY_SUFFIX) and not param.startswith(LIST):
                temp_dict[LIST + param] = list(value)
        for param in temp_dict:
            value = temp_dict[param]
            if snapshot.get(param, -1) == -1:
                snapshot[param] = value
        temp_dict.clear()

        if generate_indices(indices, snapshot):
            entities_in_indices.add(snapshot[UUID])
        else:
            skipped_entities.add(snapshot[UUID])

    return indices, entities_in_indices, skipped_entities


def iterate_over_indices(indices: dict[str, RedisDict], snapshots_dict: RedisDict[str, dict], merges: dict[str, int],
                         this_merges: dict[str, int]):
    """
    This method iterates over all the indices and finds matching entities according to the corresponding rule. These
    entities (one of which becomes a master) possess a property of locality meaning they are neighbouring within the
    index. Thus, finding candidates for merge itself is linear in time, during one run multiple merges can be done. The
    exponential nature of the algorithm hides further within the call stack.

    During the iteration the method checks if the merge candidates not connected anyhow already (A not being a master of
    B and B not being a master of A) to avoid building cyclic connections.

    Note that the method works with a view of the indices, and any entity removal from the index isn't being reflected
    in the view. This complicates the code as the check for an entity to be present in the master dataset becomes
    necessary. In particular, entities which don't have INDEX_REFS, don't belong to any index and should be skipped as
    outdated.

    :param indices: this dictionary is being modified in the call stack of this method as a side effect.
    :param snapshots_dict: this is an association of UUIDs to snapshots.
    :param merges: a merge counter by rule.
    :param this_merges: a merge counter by rule on this round.
    :return: a merge counter and a set of iterated over entities as the means of crosschecking.
    """
    merge_counter = 0
    iterated_over_set = set()

    for rule, index in indices.items():
        this_merges[rule] = 0

        if not index.values():
            continue
        last_key_to_merge_into = index.keys()[0]
        entity_to_merge_into = snapshots_dict[index.values()[0]]

        for index_key_tuple in index.keys()[1:]:

            entity_to_be_consumed_id = index.get(index_key_tuple, None)
            if entity_to_be_consumed_id is None:
                continue

            entity_to_be_consumed = snapshots_dict[entity_to_be_consumed_id]

            if is_equal_shortened_tuples(index_key_tuple, last_key_to_merge_into) and \
                    entity_to_merge_into.get(INDEX_REFS, dict()):
                if entity_to_be_consumed is not entity_to_merge_into \
                        and entity_to_be_consumed[UUID] not in entity_to_merge_into.get(DFS_SNAPSHOTS, []) \
                        and entity_to_merge_into[UUID] not in entity_to_be_consumed.get(DFS_SNAPSHOTS, []):
                    merge_counter += 1
                    merges[rule] += 1
                    this_merges[rule] += 1

                    iterated_over_set.add(entity_to_be_consumed[UUID])

                    merge(entity_to_be_consumed, entity_to_merge_into, indices, rule, index_key_tuple)

                    snapshots_dict[entity_to_merge_into[UUID]] = entity_to_merge_into
                    snapshots_dict[entity_to_be_consumed[UUID]] = entity_to_be_consumed
            else:
                entity_to_merge_into = entity_to_be_consumed
                last_key_to_merge_into = index_key_tuple

    return merge_counter, iterated_over_set


def merge(entity_to_be_consumed: dict[str, Any], entity_to_merge_into: dict[str, Any], indices: dict[str, RedisDict],
          rule: str, key: tuple):
    """
    This method merges all the values in the two entities and modifies the indices accordingly removing stale references
    and adding new.

    :param entity_to_be_consumed: this is an entity which is about to be absorbed.
    :param entity_to_merge_into: this is a master entity, it absorbs its counterpart.
    :param indices: the indices are being changed during the transformation.
    :param rule: a rule to save on which the merge is made.
    :param key: a key of the rule above.
    :return: the method mutates all the 3 parameters as side effects.
    """
    merge_or_unmerge_values_for_each_key(entity_to_be_consumed, entity_to_merge_into, True)
    add_snapshot_or_merged_master_to_new_master(entity_to_merge_into, entity_to_be_consumed, rule, key)

    remove_keys_from_indices(indices, entity_to_be_consumed)
    generate_indices(indices, entity_to_merge_into)


def add_snapshot_or_merged_master_to_new_master(entity_to_merge_into: dict[str, Any],
                                                entity_to_be_consumed: dict[str, Any],
                                                rule: str,
                                                key: tuple):
    """
    This method adds a reference to a consumed entity to the master. It also puts all the UUIDs of those entities which
    are children for the consumed entity to the new master (DFS_SNAPSHOTS).

    It also double-checks if and assures that the two entities don't already have any connectivity between them in order
    to avoid making cycles.

    :param entity_to_merge_into: this is a new master.
    :param entity_to_be_consumed: this is a consumed entity which becomes a part of the master.
    :param rule: a rule to save on which the merge is made.
    :param key: a key of the rule above.
    :return: it only modifies both entities as a side effect.
    """
    snapshots = entity_to_merge_into.get(SNAPSHOTS, list())
    edges = entity_to_merge_into.get(EDGES, list())
    entity_to_merge_into_dfs_snapshots = set(snapshots)
    entity_to_merge_into_dfs_snapshots.add(entity_to_merge_into[UUID])
    entity_to_be_consumed_dfs_snapshots = set(entity_to_be_consumed.get(DFS_SNAPSHOTS, list()))
    entity_to_be_consumed_dfs_snapshots.add(entity_to_be_consumed[UUID])
    if entity_to_be_consumed[UUID] not in entity_to_merge_into_dfs_snapshots \
            and entity_to_merge_into[UUID] not in entity_to_be_consumed_dfs_snapshots:
        snapshots.append(entity_to_be_consumed[UUID])
        edges.append((rule, key))
        entity_to_be_consumed[REF_TO_MASTER] = entity_to_merge_into[UUID]
        entity_to_merge_into[DFS_SNAPSHOTS] = entity_to_merge_into_dfs_snapshots | entity_to_be_consumed_dfs_snapshots
        entity_to_merge_into[DFS_SNAPSHOTS].discard(entity_to_merge_into[UUID])
    entity_to_merge_into[SNAPSHOTS] = snapshots
    entity_to_merge_into[EDGES] = edges


def merge_or_unmerge_values_for_each_key(entity_to_be_consumed: dict[str, Any], entity_to_merge_into: dict[str, Any],
                                         is_merge: bool):
    """
    This method actually merges or unmerges all the values for each key in the master (entity_to_merge_into), it also
    maintains and uses an auxiliary list of merged values for the sake of historical consistency as sets lose the trace
    of change.

    The data structure promises that a single value is a list and multiple values become a set. Although a list version
    of the data is maintained always as a list.

    This is a possible place of optimization as it creates new objects which is a costly memory- and time-wise
    operation.

    :param entity_to_be_consumed: this is a new master.
    :param entity_to_merge_into: this is a consumed entity which becomes a part of the master.
    :param is_merge: this method either merges or unmerges the data.
    :return: this method only mutates the aforementioned parameters as side effects.
    """
    for entity_to_be_consumed_key in entity_to_be_consumed:
        if not entity_to_be_consumed_key.endswith(KEY_SUFFIX) or entity_to_be_consumed_key.startswith(LIST):
            continue
        set_to_be_consumed = entity_to_be_consumed[entity_to_be_consumed_key]
        set_to_merge_into = entity_to_merge_into.get(entity_to_be_consumed_key, set())
        list_to_be_consumed = entity_to_be_consumed.get(LIST + entity_to_be_consumed_key, list())
        list_to_merge_into = entity_to_merge_into.get(LIST + entity_to_be_consumed_key, list())
        if is_merge:
            list_to_merge_into = list_to_merge_into + list_to_be_consumed
            entity_to_merge_into[LIST + entity_to_be_consumed_key] = list_to_merge_into
            set_to_merge_into = set_to_merge_into | set_to_be_consumed
        else:
            list_to_merge_into = list((Counter(list_to_merge_into) -
                                       Counter(entity_to_be_consumed[LIST + entity_to_be_consumed_key])).elements())
            entity_to_merge_into[LIST + entity_to_be_consumed_key] = list_to_merge_into
            set_to_merge_into = set(list_to_merge_into)
            if len(set_to_merge_into) == 0:
                entity_to_merge_into.pop(entity_to_be_consumed_key, None)
                entity_to_merge_into.pop(LIST + entity_to_be_consumed_key, None)
                continue
        entity_to_merge_into[entity_to_be_consumed_key] = set_to_merge_into


def generate_indices(indices: dict[str, RedisDict], entity_to: dict[str, Any]):
    """
    This method generates indices and puts them into the corresponding data structure.

    :param indices: the indices are being changed during the transformation.
    :param entity_to: the entity to generate indices for.
    :return: True if any new key has been generated and put into index or False if none.
    """
    result = False

    entity_to_key_set = set(entity_to.keys())
    for rule, index_to_put_keys_into in indices.items():
        rule_parameters_set = RULE_TO_SET_DICT[rule]
        combinations = generate_combinations_for_index(entity_to, entity_to_key_set, rule_parameters_set)
        result = combinations or result
        if combinations:
            remove_keys_from_index(indices, rule, entity_to)
            put_key_combination_in_index(rule, index_to_put_keys_into, entity_to, combinations)
    return result


def put_key_combination_in_index(rule: str, index_to_put_key_into: RedisDict[str, dict],
                                 entity_to_merge_into: dict[str, Any], products: list):
    """
    This method puts all the generated keys into indices. Also, it maintains the list of references to the entity in the
    list INDEX_REFS.

    :param rule: a rule for which to put the keys.
    :param index_to_put_key_into: an index to change.
    :param entity_to_merge_into: an entity for which the method puts keys in.
    :param products: a list of combinations to put into the index.
    :return: mutates index_to_put_key_into and entity_to_merge_into as side effects.
    """
    for product in products:
        for index_key in product:
            i = 0
            if index_key == ():
                continue
            j = index_to_put_key_into.bisect_left(index_key)
            if j < len(index_to_put_key_into) and is_equal_tuples(index_to_put_key_into.items()[j][0], index_key):
                i = index_to_put_key_into.items()[j][0][-1] - 1
            key = (*index_key, i)
            index_to_put_key_into[key] = entity_to_merge_into[UUID]
            refs = entity_to_merge_into.get(INDEX_REFS, dict())
            if not refs.get(rule, None):
                refs[rule] = SortedSet()
            refs[rule].add(key)
            entity_to_merge_into[INDEX_REFS] = refs


def is_equal_tuples(a: tuple, b: tuple):
    """
    This method determines if two tuples are equal except the last element of the one-element-longer one.
    :param a: the first tuple to compare.
    :param b: the second tuple to compare.
    :return: True for (1, 1, 25) and (1, 1), False for (1, 1, 1, 25) and (1, 1), False for (1, 2) and (1, 1).
    """
    for i in range(len(a) if len(a) < len(b) else len(b)):
        if isinstance(a[i], str) and isinstance(b[i], str) and \
                fuzz.token_sort_ratio(a[i], b[i]) <= LEVENSHTEIN_TOLERANCE:
            return False
        if a[i] != b[i]:
            return False
    if -1 <= len(a) - len(b) <= 1:
        return True
    return False


def is_equal_shortened_tuples(a: tuple, b: tuple):
    """
    This method checks if the tuples are equal except their last element.
    :param a: the first tuple to compare.
    :param b: the second tuple to compare.
    :return: True for (1, 1, 25) and (1, 1, 26), False for (1, 1, 1, 25) and (1, 1, 26), False for (2, 1) and (1, 1).
    """
    if len(a) != len(b):
        return False
    for i in range(len(a)):
        if isinstance(a[i], str) and isinstance(b[i], str) and \
                fuzz.token_sort_ratio(a[i], b[i]) <= LEVENSHTEIN_TOLERANCE and i != len(a) - 1:
            return False
        if a[i] != b[i] and i != len(a) - 1:
            return False
    return True


def remove_keys_from_indices(indices: dict[str, RedisDict], entity: dict[str, Any]):
    """
    It simply removes all the keys for the entity from indices.

    :param indices: the indices are being changed during the transformation.
    :param entity: the entity to remove indices for.
    :return: mutates indices and entity as side effects.
    """
    for rule, keys in entity.get(INDEX_REFS, dict()).items():
        for key in keys:
            indices[rule].pop(key, None)
    entity.pop(INDEX_REFS, None)


def remove_keys_from_index(indices: dict[str, RedisDict], rule: str, entity: dict[str, Any]):
    """
    It removes all the keys from a particular index corresponding to :param rule.

    :param indices: the indices are being changed during the transformation.
    :param rule: a rule for which to remove keys from the index.
    :param entity: the entity to remove indices for.
    :return: mutates indices and entity as side effects.
    """
    keys = entity.get(INDEX_REFS, dict()).get(rule, [])
    for key in keys:
        indices[rule].pop(key, None)
    if keys:
        entity[INDEX_REFS].pop(rule, None)


def generate_combinations_for_index(entity_to: dict[str, Any], entity_to_key_set: set[str],
                                    rule_parameters_set: set[str]):
    """
    This is an exponential time- and space-wise function which generates all the combinations for the newly created
    master according to the rule.

    If one had an index consisting of '3,6,8' and a master with 10 values for each key, then 1000 = 10 ^ 3 in total
    combinations would have been pointing to the master. If then another master was merged into the first one, and it
    had 2 values per each key, then arithmetically it ends up with 12 ^ 3 = 1728. Although the first master already had
    had 1000 index keys generated. There's a good opportunity for saving CPU cycles and generating only the additional
    728 = 1728 - 1000 keys. In cases when masters grow quite significantly by consuming snapshots one by one this trick
    makes a difference.

    :param entity_to: a master entity for which the method is about to generate keys for indices.
    :param entity_to_key_set: a key set which participates in comparison with the rule set in order to assure that all
    the keys which need to be present in the key are present in the entity as well.
    :param rule_parameters_set: a view for the rule parameters.
    :return: True if at least one key has been created, False otherwise.
    """
    if rule_parameters_set <= entity_to_key_set:
        products = []
        sets = [entity_to[entity_key] for entity_key in rule_parameters_set]
        products.append(itertools.product(*sets))
        return products
    return False


def get_key_as_int(tup: tuple):
    if tup[0].isdigit():
        return int(tup[0])
    else:
        return 0


def count_unique_master_records_and_indices(indices: dict[str, RedisDict], snapshots_dict: RedisDict[str, dict]):
    """
    This is a service method which helps to crosscheck results of deduplication.

    :param indices: this is upon what the count is performed.
    :param snapshots_dict: this is an association of UUIDs to snapshots.
    :return: the number of masters, snapshots, and index records.
    """
    masters_set = set()
    all_snapshots_set = set()
    sum_indices = 0
    index_len = 0
    for rule, index in indices.items():
        index_unique_entity_set = set()
        for key_tuple, entity_id in index.items():
            entity = snapshots_dict[entity_id]
            all_snapshots_set |= entity.get(DFS_SNAPSHOTS, set())
            masters_set.add(entity_id)
            index_unique_entity_set.add(entity_id)
        index_len = len(index.items())
        sum_indices += index_len
        print('rule: ' + str(rule) + ', index_unique_entity_set: ' +
              str(len(index_unique_entity_set)) + ', index: ' + str(index_len))
    print('intersection between snapshots and grand masters (having links from indices): '
          + str(set.intersection(all_snapshots_set, masters_set)))
    all_snapshots_including_masters_set = all_snapshots_set | masters_set
    return masters_set, sum_indices, len(all_snapshots_set), len(all_snapshots_including_masters_set), index_len


def convert_lists_to_sets(o):
    """
    As there are no standard hooks for deserializing lists as sets in json.loads() a primitive memory consuming
    converter is used as follows.
    :param o: an object to convert
    :return: a converted dictionary with sets instead of lists.
    """
    if isinstance(o, list):
        return set(o)
    elif isinstance(o, dict):
        return {k: convert_lists_to_sets(v) for k, v in o.items()}
    return o


if __name__ == '__main__':
    run(*sys.argv[1:])
